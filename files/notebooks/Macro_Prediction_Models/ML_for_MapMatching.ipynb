{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_for_MapMatching.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNnDzAI1KYxizGSvswHftPG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hdemma/hdemma.github.io/blob/master/Macro_Prediction_Models/ML_for_MapMatching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxljsHrckGM4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import pickle\n",
        "from  more_itertools import unique_everseen\n",
        "import tqdm"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq7ZSEf0ok7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Vehicle_Type = 'Electric_Vehicles'\n",
        "# Path = f'../DataSet/{Vehicle_Type}/'\n",
        "# Path = f'Post_Covid_Data/'\n",
        "Path = f''\n",
        "Vehicle_ID = [751]\n",
        "Train = False"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mttW8P0vopbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Data_Prep(df2):\n",
        "\n",
        "    df = df2.drop('Unix_Timestamp_ms', axis=1)\n",
        "    df = df.drop('F_Class', axis=1)\n",
        "    df = df.drop('Time_US_Central', axis=1)\n",
        "    df = df.drop(\"Street_Name\", axis=1)\n",
        "    df = df.drop(\"SOC\", axis=1)\n",
        "\n",
        "    df = df.dropna()\n",
        "    DataSet = np.array(df, dtype=float)\n",
        "\n",
        "    Tupple = np.shape(DataSet)  \n",
        "\n",
        "    Number_of_Records = Tupple[0]\n",
        "    Number_of_Features = Tupple[1]\n",
        "\n",
        "    print(f'\\nNumber_of_Records = {Number_of_Records}')\n",
        "    print(f'Number_of_Features = {Number_of_Features}')\n",
        "\n",
        "\n",
        "    Sample_Inputs = np.zeros((Number_of_Records, Number_of_Features - 1))\n",
        "\n",
        "    for t in range(Number_of_Records):\n",
        "        Sample_Inputs[t] = DataSet[t, :(Number_of_Features - 1)]\n",
        "\n",
        "\n",
        "    Sample_Label = np.zeros((Number_of_Records,))\n",
        "\n",
        "    for t in range(Number_of_Records):\n",
        "        Sample_Label[t] = DataSet[t][Number_of_Features - 1]\n",
        "\n",
        "    print(f'Train Label_Size = {np.shape(Sample_Label)}')\n",
        "\n",
        "    return Sample_Inputs, Sample_Label, Number_of_Features"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSwNcPJQo0wv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def DecisionTree_Model(Train):\n",
        "    if Train == True:\n",
        "\n",
        "        DT_regressor = DecisionTreeRegressor(max_depth=Number_of_Features,\n",
        "                                             splitter='best',\n",
        "                                             criterion=\"mse\", min_samples_leaf=4)\n",
        "        DT_regressor.fit(X_train, y_train)\n",
        "        y_pred = DT_regressor.predict(X_train)\n",
        "\n",
        "        MSE = np.sum(((y_train - y_pred) ** 2) / len(y_train))\n",
        "        print(f'MSE on Train = {MSE}')\n",
        "\n",
        "        y_pred = DT_regressor.predict(X_test)\n",
        "\n",
        "        MSE = np.sum(((y_test - y_pred) ** 2) / len(y_test))\n",
        "        print(f'MSE on Test = {MSE}')\n",
        "\n",
        "        RMSE = np.sqrt(np.sum(((y_test - y_pred) ** 2) / len(y_test)))\n",
        "        MAE = np.sum(abs(y_test - y_pred) / len(y_test))\n",
        "\n",
        "\n",
        "        score = DT_regressor.score(X_test, y_test)\n",
        "        print(f'Score = {score}\\n\\n')\n",
        "        score = score * 100\n",
        "\n",
        "        # save the model to disk\n",
        "        filename = '0.00065_finalized_DT_model.sav'\n",
        "        pickle.dump(DT_regressor, open(filename, 'wb'))\n",
        "    else:\n",
        "        #load the model from disk\n",
        "        filename = '0.00065_finalized_DT_model.sav'\n",
        "        loaded_model = pickle.load(open(filename, 'rb'))\n",
        "        y_pred = loaded_model.predict(X)\n",
        "        score = loaded_model.score(X,y)\n",
        "        print(f'Score = {score}')\n",
        "\n",
        "    MSE = np.sum(((y - y_pred) ** 2) / len(y))\n",
        "    print(f'MSE on Test = {MSE}')\n",
        "\n",
        "    return y_pred"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uwqs4nqCpoKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CheckErrorRate():\n",
        "    Predicted_Final_Outcome = []\n",
        "    False_Prediction = []\n",
        "    index = 0\n",
        "    while index < len(Location_Index):\n",
        "        tmp_predicted = []\n",
        "        final_outcome = []\n",
        "        tmp_actual = []\n",
        "        tmp_false = []\n",
        "        for loc_count in range(index,len(Location_Index)):\n",
        "\n",
        "            if loc_count == len(Location_Index)-1:\n",
        "                tmp_predicted.append(Predicted_Values[loc_count])\n",
        "                tmp_actual.append(Actual_Values[loc_count])\n",
        "                final_outcome.append(0)\n",
        "                tmp_false.append(0)\n",
        "                index = loc_count\n",
        "\n",
        "                m = tmp_predicted.index(max(tmp_predicted))\n",
        "                final_outcome[m] = 1\n",
        "                if final_outcome[m]!=tmp_actual[m]:\n",
        "                    tmp_false[m]=1\n",
        "                for f in final_outcome:\n",
        "                    Predicted_Final_Outcome.append(f)\n",
        "                for f in tmp_false:\n",
        "                    False_Prediction.append(f)\n",
        "                break\n",
        "\n",
        "            elif Location_Index[index]==Location_Index[loc_count]:\n",
        "                tmp_predicted.append(Predicted_Values[loc_count])\n",
        "                tmp_actual.append(Actual_Values[loc_count])\n",
        "                final_outcome.append(0)\n",
        "                tmp_false.append(0)\n",
        "\n",
        "\n",
        "            elif Location_Index[index] != Location_Index[loc_count]:\n",
        "                m = tmp_predicted.index(max(tmp_predicted))\n",
        "                final_outcome[m]=1\n",
        "                if final_outcome[m] != tmp_actual[m]:\n",
        "                    tmp_false[m] = 1\n",
        "                for f in final_outcome:\n",
        "                    Predicted_Final_Outcome.append(f)\n",
        "                for f in tmp_false:\n",
        "                    False_Prediction.append(f)\n",
        "                index = loc_count-1\n",
        "\n",
        "                break\n",
        "\n",
        "        if index>len(Location_Index):\n",
        "            break\n",
        "        index=index+1\n",
        "\n",
        "\n",
        "    ErrorPercentage = round(((sum(False_Prediction) / len(set(Location_Index))) * 100), 5)\n",
        "    Error_Rate.append(ErrorPercentage)\n",
        "    accuracy = round((100 - ErrorPercentage), 5)\n",
        "    Accuracy.append(accuracy)\n",
        "    \n",
        "    print(sum(False_Prediction), len(set(Location_Index)), ErrorPercentage, accuracy)\n",
        "    print(Error_Rate)\n",
        "    print(Accuracy)\n",
        "    return Predicted_Final_Outcome"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkCAiJP7Nc_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Final_Merge(df_pred, df2):\n",
        "    Location_Index_test = list(df_pred.Location_Index)\n",
        "    Predicted_Final_Test = list(df_pred.Predicted_Final)\n",
        "\n",
        "    Location_Index_test = list(unique_everseen(Location_Index_test))\n",
        "\n",
        "    csv = f\"{Path}{Vehicle_Name}_Data_for_Map_Matching_AfterSplit.csv\"\n",
        "    df2 = pd.read_csv(csv, low_memory=False)\n",
        "    print(f'len of df2 = {len(df2)}')\n",
        "\n",
        "    Col = ['Distance_from_location', 'Average_Forward_Location',\n",
        "           'Average_Backward_Location', 'Max_Forward_Location',\n",
        "           'Max_Backward_Location', 'Min_Forward_Location',\n",
        "           'Min_Backward_Location']\n",
        "    for col in Col:\n",
        "        df2 = df2.drop(col, axis=1)\n",
        "\n",
        "    # print(df2.columns)\n",
        "\n",
        "    Location_Index = df2.Location_Index\n",
        "\n",
        "    # print(len(set(Location_Index)))\n",
        "\n",
        "    Final_Output = df2.Final_Output\n",
        "\n",
        "    Predicted_Final = []\n",
        "\n",
        "    Location_Index_test = set(Location_Index_test)\n",
        "\n",
        "    for i in tqdm.tqdm(range(len(Location_Index))):\n",
        "        if Location_Index[i] not in Location_Index_test:\n",
        "            Predicted_Final.append(Final_Output[i])\n",
        "\n",
        "    A = Predicted_Final + Predicted_Final_Test\n",
        "\n",
        "    df2['Predicted_Final'] = A\n",
        "    Predicted_Final = df2.Predicted_Final\n",
        "\n",
        "    print(df2.columns)\n",
        "\n",
        "    df2 = df2.drop(df2[df2.Predicted_Final < 1].index)\n",
        "\n",
        "    print(len(df2))\n",
        "    csv = f\"{Path}{Vehicle_Name}_Predicted_Data_for_Map_Matching.csv\"\n",
        "    df2.to_csv(csv, index=False)\n",
        "    "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hafC-uXMpGpu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "e85c0b9b-f439-4a7d-d401-12673c33a308"
      },
      "source": [
        "for v in Vehicle_ID:\n",
        "    Vehicle_Name = f'BYD_{v}'\n",
        "    print(f'Processing {Vehicle_Name}')\n",
        "\n",
        "\n",
        "    csv = f\"{Vehicle_Name}_Data_for_Map_Matching_AfterSplit.csv\"\n",
        "    df2=pd.read_csv(csv,low_memory=False)\n",
        "    print(len(df2))\n",
        "\n",
        "    Sample_Inputs, Sample_Label, Number_of_Features = Data_Prep(df2)\n",
        "\n",
        "    X = Sample_Inputs\n",
        "    y = Sample_Label\n",
        "\n",
        "\n",
        "    Candidates = []\n",
        "    OSM_ID = []\n",
        "    Lat_Long = []\n",
        "    Location_Index = []\n",
        "\n",
        "    for i in X:\n",
        "        Point = (i[1],i[2])\n",
        "        Lat_Long.append(Point)\n",
        "        Candidates.append(i[3])\n",
        "        OSM_ID.append(i[4])\n",
        "        Location_Index.append(i[0])\n",
        "\n",
        "    X = np.delete(X, 0, 1) #\n",
        "    X = np.delete(X, 2, 1)\n",
        "\n",
        "\n",
        "    Error_Rate = []\n",
        "    Accuracy = []\n",
        "    Actual_Values = y\n",
        "    Train = False\n",
        "    Predicted_Values = DecisionTree_Model(Train)\n",
        "    Predicted_Final_Outcome= CheckErrorRate()\n",
        "\n",
        "    Predicted_Fuel_Consumption = pd.DataFrame({\"Location_Index\": Location_Index,\n",
        "                                               \"Location\": Lat_Long,\n",
        "                                               \"Candidate\": Candidates,\n",
        "                                               \"OSM_ID\": OSM_ID,\n",
        "                                               \"Predicted_Values\": Predicted_Values,\n",
        "                                               \"Predicted_Final\": Predicted_Final_Outcome,\n",
        "                                               \"Actual_Values\":Actual_Values})\n",
        "\n",
        "    csv = f\"Predicted_{Vehicle_Name}_Data_for_Map_Matching.csv\"\n",
        "\n",
        "    Predicted_Fuel_Consumption.to_csv(f'{csv}', index=False)\n",
        "    print('\\n\\n***********Mapping Done************\\n')\n",
        "\n",
        "    Final_Merge(Predicted_Fuel_Consumption,df2)\n",
        "\n",
        "    print(f'Final Merge Done for {Vehicle_Name}')\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing BYD_751\n",
            "151968\n",
            "\n",
            "Number_of_Records = 151947\n",
            "Number_of_Features = 29\n",
            "Train Label_Size = (151947,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 0.20.3 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Score = 0.874765132423057\n",
            "MSE on Test = 0.03114070138397227\n",
            "1494 81539 1.83225 98.16775\n",
            "[1.83225]\n",
            "[98.16775]\n",
            "\n",
            "\n",
            "***********Mapping Done************\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/151968 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "len of df2 = 151968\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 151968/151968 [00:02<00:00, 62944.28it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Index(['Location_Index', 'Latitude', 'Longitude', 'Unix_Timestamp_ms',\n",
            "       'Time_US_Central', 'SOC', 'Candidate', 'OSM_ID', 'Street_Name',\n",
            "       'F_Class', 'Bridge', 'Primary', 'Primary_link', 'Secondary',\n",
            "       'Secondary_link', 'Tertiary', 'Tertiary_link', 'Trunk', 'Motorway',\n",
            "       'Motorway_link', 'Service', 'Residential', 'Track', 'Unknown',\n",
            "       'Unclassified', 'Final_Count', 'Final_Output', 'Predicted_Final'],\n",
            "      dtype='object')\n",
            "81553\n",
            "Final Merge Done for BYD_751\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}